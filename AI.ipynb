{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85942448",
   "metadata": {},
   "source": [
    "We will implement a set of search strategies on the Enron data set.\n",
    "\n",
    "There is a database `enron.sqlite` in the same directory as this jupyter notebook.\n",
    "\n",
    "Connect to it, and make a data frame from the `email_text` column. Use just the \"ham\"\n",
    "documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0ba81b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2408</th>\n",
       "      <td>Subject: hpl meter # 987195 tatton central poi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>Subject: cpr pipeline exchange activity report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>Subject: king ranch gas plant - 12 / 2000 elec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>Subject: duplicates\\nsorry for any duplicate ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2873</th>\n",
       "      <td>Subject: hpl nom for march 17 , 2001\\n( see at...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             email_text\n",
       "2408  Subject: hpl meter # 987195 tatton central poi...\n",
       "1753  Subject: cpr pipeline exchange activity report...\n",
       "1198  Subject: king ranch gas plant - 12 / 2000 elec...\n",
       "2007  Subject: duplicates\\nsorry for any duplicate ,...\n",
       "2873  Subject: hpl nom for march 17 , 2001\\n( see at..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "import sqlite3\n",
    "conn = sqlite3.connect(\"enron.sqlite\")\n",
    "df = pandas.read_sql(\"select email_text from enron where spam_or_ham = 'ham'\", conn)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b68e485",
   "metadata": {},
   "source": [
    "## Exact substring match\n",
    "\n",
    "Let's be super-naive first. Create a function that takes a string and a Series\n",
    "as argument. It should search through the  dataframe looking for it anywhere in the email_text. Have it\n",
    "return a boolean Series aligned with the dataframe. (i.e. it should return a series\n",
    "with True if that row was a match, and False if it wasn't). We'll use this \n",
    "pattern in other search methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "221dcfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def substring_match(series, string_to_find):\n",
    "    return series.str.contains(string_to_find)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924d6525",
   "metadata": {},
   "source": [
    "Test it out. Try the word `energy` for example. You can use\n",
    "`df[your_search_func(df.email_text, \"energy\")].email_text` to display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb00dbe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9       Subject: sarco lateral and crow o ' connor met...\n",
       "29      Subject: energy operations promotions\\ni am pl...\n",
       "35      Subject: revised nom for copano ' s . . . smal...\n",
       "39      Subject: re : no / actual vols for 5 / 22 / 01...\n",
       "40      Subject: meter 981594 - san jacinto low pressu...\n",
       "                              ...                        \n",
       "3647    Subject: new noms\\n- - - - - - - - - - - - - -...\n",
       "3648    Subject: seacrest meter # 0435 - april , 2001\\...\n",
       "3651    Subject: oct noms\\n- - - - - - - - - - - - - -...\n",
       "3666    Subject: 6 th noms\\n- - - - - - - - - - - - - ...\n",
       "3671    Subject: pennzenergy property details\\n- - - -...\n",
       "Name: email_text, Length: 415, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[substring_match(df.email_text,'energy')].email_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a413c0",
   "metadata": {},
   "source": [
    "What should we do about ranking?\n",
    "\n",
    "The more of the email that our search string covers, the more likely it is to be useful.\n",
    "So that should return a small number. If our search string is a tiny part of the email,\n",
    "that should return a big number.\n",
    "\n",
    "Write a function that takes a pandas Series (of emails) and a string and returns the\n",
    "number of times bigger the email is compared to the string. \n",
    "\n",
    "(If the search string has zero length, it's meaningless as a search, so let's ignore that.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26828f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def substring_match_ranking(series, string_to_find):\n",
    "    return series.str.len() / len(string_to_find)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ec92dc",
   "metadata": {},
   "source": [
    "What should we do about creating a displayable snippet?\n",
    "\n",
    "Let's break it into two parts. First, a function that takes a \n",
    "successfully-matched string, and the string to highlight, \n",
    "and returns some Markdown just for that.\n",
    "\n",
    "For example, if you are searching for `six` in `The sixth sick sheik's sixth sick sheep.`\n",
    "it should return `The  **six** th sick sheik's  **six** th sick sheep.`\n",
    "\n",
    "(A really good algorithm would clean up any email that had Markdown elements in it first. But\n",
    "we won't worry about that too much.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8ef7cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The  **six** th sick sheik's  **six** th sick sheep.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def highlight_simple_substring(corpus_text, string_to_find):\n",
    "    return f' **{string_to_find}** '.join(corpus_text.split(string_to_find))\n",
    "\n",
    "highlight_simple_substring(\"The sixth sick sheik's sixth sick sheep.\", 'six')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340a0c14",
   "metadata": {},
   "source": [
    "Secondly, let's make a general purpose `snippet_viewer`. It should take\n",
    "a `ranking_series` and a `snippet_series` and create a Markdown document\n",
    "out of the snippets, starting with the highest-ranked, working down. Just\n",
    "show the top three results.\n",
    "\n",
    "Then it can use IPython.display.Markdown to show the document nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd73ce96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Document 1\n",
       "\n",
       "_Best search result_\n",
       "\n",
       "### Document 0\n",
       "\n",
       "Middle **search result**\n",
       "\n",
       "### Document 3\n",
       "\n",
       "Last result shown\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display\n",
    "\n",
    "def snippet_viewer(ranking_series, snippet_series):\n",
    "    answer = \"\"\n",
    "    i = 0\n",
    "    for idx in ranking_series.sort_values().index:\n",
    "        snippet = snippet_series.loc[idx]\n",
    "        answer += f\"### Document {idx}\\n\\n{snippet}\\n\\n\"\n",
    "        i += 1\n",
    "        if i == 3:\n",
    "            break\n",
    "    return IPython.display.Markdown(answer)\n",
    "    \n",
    "snippet_viewer(pandas.Series([1,0,3,2]), \n",
    "               pandas.Series(['Middle **search result**', '_Best search result_', '#### Worst search result',\n",
    "                             'Last result shown']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3c082f",
   "metadata": {},
   "source": [
    "------------------\n",
    "\n",
    "# Testing out what we've done so far\n",
    "\n",
    "Create a variable for the search term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a09764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = 'cat'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf994adc",
   "metadata": {},
   "source": [
    "Create a pandas Series which contains only successful search results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d367819",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = df[substring_match(df.email_text, search_term)].email_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30ced42",
   "metadata": {},
   "source": [
    "Calculate ranks for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "595e6b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_ranking = substring_match_ranking(search_results, search_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a58593",
   "metadata": {},
   "source": [
    "Create a series containing snippets of the search results (using the highlighter you wrote a moment ago)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7653865",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_snippets = search_results.map(lambda x: highlight_simple_substring(x, search_term))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9a4eef",
   "metadata": {},
   "source": [
    "Call the snippet viewer function with your ranking and snippet Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8a4cd43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Document 2919\n",
       "\n",
       "Subject: is this fri feb 11 a problem for taking va **cat** ion ?\n",
       "\n",
       "\n",
       "### Document 2007\n",
       "\n",
       "Subject: dupli **cat** es\n",
       "sorry for any dupli **cat** e , having problems with lotus\n",
       "notes .\n",
       "gary green\n",
       "\n",
       "### Document 2689\n",
       "\n",
       "Subject: june va **cat** ion\n",
       "please submit your june va **cat** ion to me asap .\n",
       "thank you !\n",
       "yvette\n",
       "x 3 . 5953\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snippet_viewer(search_ranking, search_snippets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60db09a2",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "# Improvements\n",
    "\n",
    "It would be nice to be able to search case-insensitively, to get whole words, and it would be nice to search for multiple terms in an email.\n",
    "\n",
    "That means we'll need a query interpreter (there was no point in having one in the previous section). Fortunately,\n",
    "this query interpreter should be pretty simple: take a string, lowercase it, and split it up into words.\n",
    "\n",
    "We can use the NLTK library (`nltk.word_tokenize()` to split the sentences up into words.\n",
    "\n",
    "Write the multiword query interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abe62cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiword_query_interpreter(search_term):\n",
    "    return [x.lower() for x in nltk.word_tokenize(search_term)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4577f1cf",
   "metadata": {},
   "source": [
    "As part of the indexing (data preparation) step, we'll need to apply the corpus of emails in lowercase\n",
    "and word-separated as well. Create a new column in the dataframe for this.\n",
    "\n",
    "The result should look like this:\n",
    "\n",
    "```\n",
    "0       [subject, :, tue, ,, 23, mar, 2004, 12, :, 06,...\n",
    "1       [subject, :, slutty, milf, wants, to, meet, yo...\n",
    "2       [subject, :, better, s, ., e, ., x, guar, ., a...\n",
    "3       [subject, :, urgent, message, mr, francis, oma...\n",
    "4       [subject, :, do, you, feel, safe, as, an, amer...\n",
    "```\n",
    "\n",
    "If you're feeling lazy, you could observe that the query interpreter would do the job for you.\n",
    "(It's not common that the data prep and query interpreter are the same.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf3d0853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [subject, :, hpl, nom, for, may, 4, ,, 2001, (...\n",
       "1       [subject, :, january, -, meter, 2186, clear, l...\n",
       "2       [subject, :, revised, :, eastrans, nomination,...\n",
       "3       [subject, :, re, :, fuel, application, of, the...\n",
       "4       [subject, :, re, :, nominations, we, agree, ''...\n",
       "                              ...                        \n",
       "3667    [subject, :, bayer, -, march, 2001, volumes, j...\n",
       "3668    [subject, :, revision, #, 1, -, hpl, nom, for,...\n",
       "3669    [subject, :, re, :, hpl, discrepancy, is, this...\n",
       "3670    [subject, :, daren, ,, equistar, will, be, bri...\n",
       "3671    [subject, :, pennzenergy, property, details, -...\n",
       "Name: lowercase_words, Length: 3672, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "df['lowercase_words'] = df.email_text.map(multiword_query_interpreter)\n",
    "df.lowercase_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b16e7b",
   "metadata": {},
   "source": [
    "Now write a function that takes a lower case word and searches through a Series of \n",
    "lowercase'd words, reporting True for the elements where it is present, and False otherwise.\n",
    "\n",
    "So if you search for the word `cat` in this:\n",
    "```\n",
    "pandas.Series([['cat', 'banana', 'crab'], \n",
    "               ['cat', 'dog', 'elephant'], \n",
    "               ['frog', 'goat']]\n",
    "             )\n",
    "```\n",
    "it should return:\n",
    "```\n",
    "0     True\n",
    "1     True\n",
    "2    False\n",
    "dtype: bool\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d001b705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     True\n",
       "1     True\n",
       "2    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exact_word_search(lowercase_wordlist_series, word_to_search_for):\n",
    "    def word_is_present(where):\n",
    "        return word_to_search_for in where\n",
    "    return lowercase_wordlist_series.map(word_is_present)\n",
    "\n",
    "exact_word_search(pandas.Series([['cat', 'banana', 'crab'], \n",
    "                                 ['cat', 'dog', 'elephant'], \n",
    "                                 ['frog', 'goat']]), \n",
    "                  'cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f312c079",
   "metadata": {},
   "source": [
    "Now write a function that can take a wordlist series, and a list of lower case words and find places in \n",
    "the wordlist series where all of them are present.\n",
    "\n",
    "Remeber that `&` can be used on a pair of pandas Series objects to do a logical `and` on each\n",
    "element.\n",
    "\n",
    "The result of search for `['cat', 'crab']` in this series\n",
    "```\n",
    "pandas.Series([['cat', 'banana', 'crab'], \n",
    "               ['cat', 'dog', 'elephant'], \n",
    "               ['frog', 'goat']]\n",
    "             )\n",
    "```\n",
    "should be\n",
    "```\n",
    "0     True\n",
    "1    False\n",
    "2    False\n",
    "dtype: bool\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84e2059a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     True\n",
       "1    False\n",
       "2    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multiword_exact_search(lowercase_wordlist_series, words_to_search_for):\n",
    "    survivors = pandas.Series(index=lowercase_wordlist_series.index, data=True)\n",
    "    for word_to_search_for in words_to_search_for:\n",
    "        survivors = survivors & exact_word_search(lowercase_wordlist_series, word_to_search_for)\n",
    "    return survivors\n",
    "\n",
    "multiword_exact_search(pandas.Series([['cat', 'banana', 'crab'], \n",
    "                                 ['cat', 'dog', 'elephant'], \n",
    "                                 ['frog', 'goat']]), \n",
    "                  ['cat', 'crab'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d534b2c9",
   "metadata": {},
   "source": [
    "There are many ways to rank multi-word searches. Usually the key is to look for documents where\n",
    "the words appear close together.\n",
    "\n",
    "For now, let's assume that the words in a multi-word search only appear once each. Then the\n",
    "standard deviation of their positions in the document captures the idea of closeness --- a\n",
    "smaller standard deviation puts them closer together and we want smaller to mean \"better match\".\n",
    "\n",
    "If there is only one distinct word, the standard deviation isn't well-defined, but\n",
    "we could use the length of the email divided by the number of occurences as the ranking.\n",
    "A shorter email that uses that word many times is likely to be a good candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41e64fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiword_ranking(lowercase_wordlist_series, words_to_search_for):\n",
    "    if len(words_to_search_for) == 1:\n",
    "        return (lowercase_wordlist_series.map(len) /\n",
    "                lowercase_wordlist_series.map(lambda x: len([t for t in x if t in words_to_search_for])))\n",
    "    def stddev_ranker(where):\n",
    "        return pandas.Series([i for (i,w) in enumerate(where) if w in words_to_search_for]).std()\n",
    "    return lowercase_wordlist_series.map(stddev_ranker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084f1fd2",
   "metadata": {},
   "source": [
    "Create a snippet display function: **bold**-ify any word that is in the search terms. If you feel\n",
    "like being fancy, you could make sure that _, * and $ are backslash escaped too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d74f2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This **is** a **sample** sentence'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def highlight_multiword_ranking(email_word_list, words_to_search_for):\n",
    "    def boldify(word):\n",
    "        if word in words_to_search_for:\n",
    "            return f\"**{word}**\"\n",
    "        else:\n",
    "            return word.replace('_', '\\_').replace('*', '\\*').replace('$', '\\$')\n",
    "    return \" \".join([boldify(x) for x in email_word_list])\n",
    "\n",
    "highlight_multiword_ranking(['This', 'is', 'a', 'sample', 'sentence'], ['is', 'sample'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62504b4",
   "metadata": {},
   "source": [
    "### Putting it all together.\n",
    "\n",
    "This will look a lot like the process we did in the previous section.\n",
    "\n",
    "- Set a variable for the search term (so that you can rerun different searches easily)\n",
    "\n",
    "- Process that search term using the query interpreter\n",
    "\n",
    "- Pass the interpreted query to your multi-word exact match search (remember to use the lowercase'd words as the series to view)\n",
    "\n",
    "- Take the result of that and run it through the ranking algorithm and the snippet maker\n",
    "\n",
    "- Put those results through the snippet viewer you created in the previous section (it should still work here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d6c4d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Document 1849\n",
       "\n",
       "subject : defs 2001 i have some changes to the defs deals for 2001 . we need to add demand fees for the over delivery and excess **charges** . i have attached the spreadsheets in case you need them . prod . deal demand fee feb 2001 157278 \\$ 11 , 903 . 17 march 2001 157278 \\$ 294 . 85 april 2001 229758 \\$ 308 . 53 thanks , megan\n",
       "\n",
       "### Document 1441\n",
       "\n",
       "subject : defs may 2001 daren : please enter a demand fee on deal 157278 for may 2001 in the amount of \\$ 369 . 69 . we need to bill defs for the remaining excess and over delivery **charges** . also , i was going back over my calc sheets and i found an error in oct 2000 . please enter a demand fee for \\$ 647 . 35 on deal 157278 for oct 2000 . thanks , megan\n",
       "\n",
       "### Document 2158\n",
       "\n",
       "subject : duke exchange deal daren : i have several months that need to have the demand **charges** either added or adjusted . when katherine gave you the numbers the first time , there were some spot deals included in the exchange deals . the volumes have now been moved to the correct deals and the demand **charges** need to be corrected . i have listed the changes below . let me know if you would like to see the spreadsheets . deal 157278 3 / 00 add demand charge of \\$ 73 , 403 . 47 for excess charge 4 / 00 change demand fee from \\$ 1 , 507 . 56 to \\$ 1 , 966 . 93 6 / 00 change demand fee from \\$ 1 , 129 . 99 to \\$ 359 . 97 deal 157288 3 / 00 change demand fee from \\$ 3 , 526 . 98 to \\$ 245 . 82 thanks , megan\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_term = \"charges\"\n",
    "interpreted = multiword_query_interpreter(search_term)\n",
    "relevant_emails = df.lowercase_words[multiword_exact_search(df.lowercase_words, interpreted)]\n",
    "ranked_emails = multiword_ranking(relevant_emails, interpreted)\n",
    "snippets = relevant_emails.map(lambda x: highlight_multiword_ranking(x, interpreted))\n",
    "snippet_viewer(ranked_emails, snippets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fcdc9c",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "# Lemmatized search\n",
    "\n",
    "It should be smart enough to understand *charge* and *charges* are the same thing. For this\n",
    "we need to *lemmatize* each word. \n",
    "\n",
    "The NLTK library has a `nltk.stem.WordNetLemmatizer()` class with a `.lemmatize()` method\n",
    "we can use for this. Remember that we will need to lemmatize the query term as well as the\n",
    "corpus of emails.\n",
    "\n",
    "You might need to download some NLTK models.\n",
    "```\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca4b6887",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/gregb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/gregb/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/gregb/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Create lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651ac7c3",
   "metadata": {},
   "source": [
    "Do you data preparation and make your query interpreter.\n",
    "\n",
    "You will probably find that the search code and ranking code are the same as the\n",
    "previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9aee0ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizing_query(search_term):\n",
    "    return [lemmatizer.lemmatize(x).lower() for x in nltk.word_tokenize(search_term)]\n",
    "\n",
    "df['lemmatized_words'] = df.email_text.map(lemmatizing_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a624463b",
   "metadata": {},
   "source": [
    "The snippet highlighter is trickier. We might need to highlight a word that wasn't exactly the\n",
    "way it was in the search query. A neat way to resolve this is to find words in the lemmatized\n",
    "word lists that matched, and then highlight the text in the unlemmatized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "979dc0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def higlighter_for_lemmatized_query(unlemmatized_word_list, lemmatized_word_list, terms_to_search_for):\n",
    "    def boldify(unlemmatized_word, lemmatized_word):\n",
    "        if lemmatized_word in terms_to_search_for:\n",
    "            return f\"**{unlemmatized_word}**\"\n",
    "        else:\n",
    "            return unlemmatized_word.replace('_', '\\_').replace('*', '\\*').replace('$', '\\$')\n",
    "    return \" \".join([boldify(x,y) for (x,y) in zip(unlemmatized_word_list,unlemmatized_word_list)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8215ab37",
   "metadata": {},
   "source": [
    "When you search for `charges` or `charge` you should get the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06eb2a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Document 138\n",
       "\n",
       "subject : pg & e texas pipeline kellie , pg & e will probably try to bill u for parking a volume of 10 , 263 for august 8 and 9 at \\$ . 03 / mmbtu each day . please do not pay this **charge** when we receive this invoice . pg & e wa unable to make delivery into el paso because of high sulfur content in their gas and is trying to **charge** u with a parking **charge** for gas that they could not deliver . please let me know if you need any additional information . thanks .\n",
       "\n",
       "### Document 88\n",
       "\n",
       "subject : dec 2000 prod : panther pipeline demand **charge** please let me know if this is a vaild demand **charge** . the deal is under a gtc contract and daren indicated that he wa not aware of this deal unless it ha something to do with entex . fyi the volume flow at this meter ( # 981598 ) for dec 2000 wa zero . please advise - katherine\n",
       "\n",
       "### Document 1572\n",
       "\n",
       "subject : duke energy field 9 / 00 please add the demand **charge** for excess fee for 9 / 00 on sale deal 157278 in the amount of \\$ 1 , 175 . 51 . thanks , megan\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_term = \"charge\"\n",
    "interpreted = lemmatizing_query(search_term)\n",
    "search_matches = multiword_exact_search(df.lemmatized_words, interpreted)\n",
    "relevant_lemmatized_emails = df.lowercase_words[search_matches]\n",
    "ranked_emails = multiword_ranking(relevant_lemmatized_emails, interpreted)\n",
    "relevant_unlemmatized_emails = df.lemmatized_words[search_matches]\n",
    "relevant_emails = pandas.DataFrame({'unlemmatized': relevant_unlemmatized_emails,\n",
    "                                   'lemmatized': relevant_lemmatized_emails})\n",
    "snippets = relevant_emails.apply(\n",
    "    lambda row: higlighter_for_lemmatized_query(row['unlemmatized'], row['lemmatized'], interpreted),\n",
    "    axis=1)\n",
    "snippet_viewer(ranked_emails, snippets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fe6da6",
   "metadata": {},
   "source": [
    "# Bag-of-words vectorization\n",
    "\n",
    "For search, a bag-of-words and bag-of-ngrams vectorization can be quite effective. Often this is done at\n",
    "the sentence level (high ranking) and also at the document level (low ranking). We'll just do sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77301aef",
   "metadata": {},
   "source": [
    "Create a new dataframe, which has each sentence from the original email dataframe as a separate row, and\n",
    "has a cross-reference back to the original dataframe's index. NLTK has a `nltk.sent_tokenize()` function\n",
    "that will be helpful for this.\n",
    "\n",
    "It's often nice to have a sentence-number-within-email number as well. This is useful in snippet\n",
    "creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02f23f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crossref</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentence_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17064</th>\n",
       "      <td>2047</td>\n",
       "      <td>2 . change the meter # and drn # at facility #...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14521</th>\n",
       "      <td>1750</td>\n",
       "      <td>the second will\\nimmediately follow .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11602</th>\n",
       "      <td>1393</td>\n",
       "      <td>18 or $ 33 , 022 .</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11826</th>\n",
       "      <td>1415</td>\n",
       "      <td>960 &amp; 18 , 218 mmbtu @ $ 4 .</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>215</td>\n",
       "      <td>once the 6 \" line has been pigged , the produc...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       crossref                                           sentence  \\\n",
       "17064      2047  2 . change the meter # and drn # at facility #...   \n",
       "14521      1750              the second will\\nimmediately follow .   \n",
       "11602      1393                                 18 or $ 33 , 022 .   \n",
       "11826      1415                       960 & 18 , 218 mmbtu @ $ 4 .   \n",
       "1725        215  once the 6 \" line has been pigged , the produc...   \n",
       "\n",
       "       sentence_number  \n",
       "17064               11  \n",
       "14521                1  \n",
       "11602               20  \n",
       "11826               26  \n",
       "1725                 9  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df_prep = []\n",
    "for idx, row in df.iterrows():\n",
    "    sentences = nltk.sent_tokenize(row['email_text'])\n",
    "    for sentence_number, sentence in enumerate(sentences):\n",
    "        sentence_df_prep.append({'crossref': idx, 'sentence': sentence, 'sentence_number': sentence_number})\n",
    "sentence_df = pandas.DataFrame.from_records(sentence_df_prep)\n",
    "sentence_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1449090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31705, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b340cdb2",
   "metadata": {},
   "source": [
    "Calculate tfidf vectors for the sentences in this sentence dataframe. In an ideal world, you \n",
    "would want to use BM25 instead of tfidf, but neither Keras nor scikit-learn offers this as \n",
    "an option.\n",
    "\n",
    "Including bigrams helps: when someone searches for a two-word phrase that appears a few times\n",
    "in the corpus, it will get a huge boost.\n",
    "\n",
    "Vectorization adaption may be a slow process: expect to wait a minute or two for it to complete.\n",
    "\n",
    "If you have a GPU enabled, and your GPU doesn't have much memory, you might need to limit the vocabulary\n",
    "size. (e.g. with `keras.layers.TextVectorization(max_tokens=20000, output_mode='tf_idf', ngrams=2)`).\n",
    "In real life you wouldn't limit the vocabulary, since often someone wants to search for the document\n",
    "that has the word \"...\" in it.\n",
    "\n",
    "You will need to keep the tokenizer around for your query processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d72520fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 00:08:30.497429: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-10-09 00:08:30.497534: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "bow_vectorizer = keras.layers.TextVectorization(output_mode='tf_idf',\n",
    "                                                #ngrams=2,\n",
    "                                                max_tokens=20000\n",
    "                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3aa28df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 00:08:30.549236: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-10-09 00:08:30.613795: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.9 s, sys: 12.4 s, total: 30.4 s\n",
      "Wall time: 27.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bow_vectorizer.adapt(sentence_df.sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad99ed2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 296 ms, sys: 1.25 s, total: 1.55 s\n",
      "Wall time: 1.91 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[UNK]</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>ect</th>\n",
       "      <th>for</th>\n",
       "      <th>and</th>\n",
       "      <th>hou</th>\n",
       "      <th>enron</th>\n",
       "      <th>subject</th>\n",
       "      <th>on</th>\n",
       "      <th>...</th>\n",
       "      <th>135958</th>\n",
       "      <th>135895</th>\n",
       "      <th>135842</th>\n",
       "      <th>135708</th>\n",
       "      <th>1350</th>\n",
       "      <th>134987</th>\n",
       "      <th>134755</th>\n",
       "      <th>1344</th>\n",
       "      <th>1343</th>\n",
       "      <th>1342</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9496</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28795</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.3797</td>\n",
       "      <td>1.396265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.396265</td>\n",
       "      <td>2.950585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20232</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.779899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30240</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 20000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       [UNK]     the        to       ect       for  and  hou  enron  subject  \\\n",
       "9496     0.0  0.0000  0.000000  0.000000  0.000000  0.0  0.0    0.0      0.0   \n",
       "28795    0.0  5.3797  1.396265  0.000000  0.000000  0.0  0.0    0.0      0.0   \n",
       "1125     0.0  0.0000  1.396265  2.950585  0.000000  0.0  0.0    0.0      0.0   \n",
       "20232    0.0  0.0000  0.000000  0.000000  1.779899  0.0  0.0    0.0      0.0   \n",
       "30240    0.0  0.0000  0.000000  0.000000  0.000000  0.0  0.0    0.0      0.0   \n",
       "\n",
       "        on  ...  135958  135895  135842  135708  1350  134987  134755  1344  \\\n",
       "9496   0.0  ...     0.0     0.0     0.0     0.0   0.0     0.0     0.0   0.0   \n",
       "28795  0.0  ...     0.0     0.0     0.0     0.0   0.0     0.0     0.0   0.0   \n",
       "1125   0.0  ...     0.0     0.0     0.0     0.0   0.0     0.0     0.0   0.0   \n",
       "20232  0.0  ...     0.0     0.0     0.0     0.0   0.0     0.0     0.0   0.0   \n",
       "30240  0.0  ...     0.0     0.0     0.0     0.0   0.0     0.0     0.0   0.0   \n",
       "\n",
       "       1343  1342  \n",
       "9496    0.0   0.0  \n",
       "28795   0.0   0.0  \n",
       "1125    0.0   0.0  \n",
       "20232   0.0   0.0  \n",
       "30240   0.0   0.0  \n",
       "\n",
       "[5 rows x 20000 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sentence_vectors = pandas.DataFrame(\n",
    "    data=bow_vectorizer(sentence_df.sentence),\n",
    "    index=sentence_df.index,\n",
    "    columns=bow_vectorizer.get_vocabulary()\n",
    ")\n",
    "sentence_vectors.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8147728e",
   "metadata": {},
   "source": [
    "Your query interpreter will take some text, and pass it through the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e1206b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20000,), dtype=float32, numpy=array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bow_query_interpreter(search_term):\n",
    "    return bow_vectorizer(search_term)\n",
    "\n",
    "bow_query_interpreter(\"charges for gas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e70c197",
   "metadata": {},
   "source": [
    "Search and ranking can often be joined into one step: simply ask about the cosine similarity between\n",
    "the query vector and the sentences in the email.\n",
    "\n",
    "There are many libraries that include functionality for calculating the cosine similarity.\n",
    "\n",
    " - `sklearn.metrics.pairwise.cosine_similarity()`\n",
    " - `keras.losses.CosineSimilarity(reduction=tf.keras.losses.Reduction.NONE)`\n",
    " \n",
    "Whatever way you choose, calculate the cosine similarity, find the sentences that match\n",
    "most closely, and return the references to the top email documents (and which sentence\n",
    "was the hit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b98f0728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_scores</th>\n",
       "      <th>best_sentence_number</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crossref</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.606621</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2906</th>\n",
       "      <td>-0.606621</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>-0.422475</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          document_scores  best_sentence_number\n",
       "crossref                                       \n",
       "15              -0.606621                    24\n",
       "2906            -0.606621                    21\n",
       "216             -0.422475                     8"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras.losses\n",
    "import tensorflow as tf\n",
    "\n",
    "def bow_query_search_and_ranking_function(sentence_vectors, query_vector):\n",
    "    cosine_loss = keras.losses.CosineSimilarity(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    similarity = cosine_loss(query_vector, sentence_vectors)\n",
    "    sentence_indexes = pandas.DataFrame({'crossref': sentence_df.crossref, \n",
    "                                         'sentence_number': sentence_df.sentence_number,\n",
    "                                         'sentence': sentence_df.sentence,\n",
    "                                         'similarity': similarity})\n",
    "    sentence_indexes = sentence_indexes[sentence_indexes.similarity.abs() > 0.0001]\n",
    "    best_documents = sentence_indexes.groupby('crossref').similarity.min().nsmallest(3)\n",
    "    best_sentences = sentence_indexes[sentence_indexes.crossref.isin(list(best_documents.index))\n",
    "                                     ].set_index('sentence_number').groupby('crossref').similarity.idxmin()\n",
    "    return pandas.DataFrame({'document_scores': best_documents, \n",
    "                             'best_sentence_number': best_sentences}).sort_values('document_scores')\n",
    "\n",
    "bow_query_search_and_ranking_function(sentence_vectors, \n",
    "                                     bow_query_interpreter(\"apache agreement\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc93e52",
   "metadata": {},
   "source": [
    "We could get smart and display the words that were matched, but since we're operating at the sentence\n",
    "level, we'll just make the right sentence bold. Create a highlight function that takes\n",
    "an email and a sentence number and returns Markdown that highlights that sentence in the email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "999b4639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_bow(email, highlight_sentence_number):\n",
    "    def boldify(this_sentence_number, sentence):\n",
    "        if this_sentence_number == highlight_sentence_number:\n",
    "            return f\"**{sentence}**\"\n",
    "        else:\n",
    "            return sentence.replace('*', '\\*').replace('$', '\\$').replace('_', '\\_')\n",
    "    return \" \".join([boldify(i,s) for i,s in enumerate(nltk.sent_tokenize(email))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bb5a39",
   "metadata": {},
   "source": [
    "Let's pull it all together into one function:\n",
    "\n",
    "- Take a search term, bag-of-words vectorize it\n",
    "\n",
    "- Simultaneously search-and-rank that vectorized search term against the vectorized sentences\n",
    "\n",
    "- Select the relevant documents from the results\n",
    "\n",
    "- Make a Markdown snippet for them\n",
    "\n",
    "- Display the resulting Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9115ea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_search(search_term):\n",
    "    query_vector = bow_query_interpreter(search_term)\n",
    "    results = bow_query_search_and_ranking_function(sentence_vectors, query_vector)\n",
    "    markdown_output = \"\"\n",
    "    for idx,row in results.iterrows():\n",
    "        email = df.loc[idx].email_text\n",
    "        markdown_output += f\"### Document {idx}\\n\\n\"\n",
    "        markdown_output += highlight_bow(email, row['best_sentence_number'])\n",
    "        markdown_output += '\\n\\n'\n",
    "    return IPython.display.Markdown(markdown_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adc4def",
   "metadata": {},
   "source": [
    "Try it out! Some interesting phrases to search for:\n",
    "\n",
    "- apache agreement\n",
    "\n",
    "- contract volumes\n",
    "\n",
    "- elephant\n",
    "\n",
    "`Elephant` is out of vocabulary, so it matches the documents with the most out-of-vocabulary content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb519276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Document 1369\n",
       "\n",
       "Subject: spinaker / n . padre island block 883 : allocations\n",
       "per our various discussions , i am sending an email to reiterate the\n",
       "disposition of volumes at meter 9862 , the lehman spinaker pay meter . at this\n",
       "time , the general land office is transporting their share of production\n",
       "( 21 . 8945 % interest ) and hpl is purchasing the remaining 78 . **1055 % .** the glo has nominated a transport volume at meter 9848 , effective september\n",
       "1 , 2000 . during this time , some test gas flowed . i have had the hpl\n",
       "purchase back - dated to coincide with the transport nomination , and have had\n",
       "both deals moved to 9862 . the meter should probably be designated a\n",
       "callout / swing so that the proper equity percentages can be allocated after\n",
       "the production month by volume management . for october production forward , i am going to re - rank / confirm the pay meter\n",
       "to ensure that the glo receives a volume which should be close to their\n",
       "percentage after actuals close for the month . this should alleviate any\n",
       "large balance swings on their agreement . anita : for september , i need an accounting arrangement on deal ticket\n",
       "379424 ; hpl gathering agmt . at meter 9862 . please unallocate the transport\n",
       "and purchase at 9848 . the meter is a daily swing right now . let me know\n",
       "when you are ready and i will change the allocation methodology . i apologize for the length of this email , i want to make sure that we are all\n",
       "on the same page , to the extent that we need to be , prior to this deal\n",
       "getting too far down the road . please do not hesitate to call me if you have\n",
       "any questions , comments , concerns , etc . i am at extension 35251 .\n",
       "thank you all for your time and patience ,\n",
       "mary\n",
       "\n",
       "### Document 3171\n",
       "\n",
       "Subject: fw : \" red , white and blue out \"\n",
       "- - - - - original message - - - - -\n",
       "from : carter , rhonda [ mailto : rcarter @ cooperinst . org ]\n",
       "sent : friday , september 14 , 2001 12 : 33 pm\n",
       "to : ' al \\_ abbott @ compuserve . com ' ; ' mabner @ sprintmail . com ' ;\n",
       "' aggiebob @ hotmail . com ' ; ' adamsck @ flash . net ' ; ' gadams @ promus . com ' ;\n",
       "' pjadell @ yahoo . com ' ; ' bob @ cybersitebuilders . com ' ;\n",
       "' worml 998 @ hotmail . com ' ; ' janie . beth @ prodigy . net ' ; ' gakin @ mccarthy . com ' ;\n",
       "' vja @ flash . net ' ; ' locke . alder @ gte . net ' ; ' calexaol @ 7 - 11 . com ' ;\n",
       "' erika @ publish . no . irs . gov ' ; ' ali @ buz . net ' ; ' brada @ ticnet . com ' ;\n",
       "' svallen @ aol . com ' ; ' jand 30 @ aol . com ' ; ' allan @ stratsolgroup . com ' ;\n",
       "' chuck \\_ anderson @ oxy . com ' ; ' mdqsga 96 @ aol . com ' ;\n",
       "' brian \\_ anhalt @ bigfoot . com ' ; ' aranda @ nbstx . com ' ; ' aggiemom @ archer . cx ' ;\n",
       "' jard @ nortelnetworks . com ' ; ' abarch @ airmail . net ' ; ' narguello @ yahoo . com ' ;\n",
       "' jarmstrong @ tqtx . com ' ; ' mikie @ aggie . zzn . com ' ; ' ag 85 @ home . com ' ;\n",
       "' kmarnold @ home . com ' ; ' hollya @ cyber - designs . com ' ;\n",
       "' hughashburn @ netscape . net ' ; ' bob @ cybersitebuilders . com ' ;\n",
       "' olinatkinson @ dellnet . com ' ; ' papaayres @ aol . com ' ; ' abackof 68 @ aol . com ' ;\n",
       "' badgett @ ti . com ' ; ' kbailie @ nortel . com ' ; ' wjbaird @ mapsco . com ' ;\n",
       "' jbaker @ ecomtrading . com ' ; ' tim . banigan @ nortelnetworks . com ' ;\n",
       "' atbarlow @ mail . smu . edu ' ; ' arnonvic @ aol . com ' ;\n",
       "' john \\_ laurabarr @ email . msn . com ' ; ' jillmbarrow @ hotmail . com ' ;\n",
       "' b - barton @ ti . com ' ; ' tbates @ why . net ' ; ' normabautista @ worldnet . att . net ' ;\n",
       "' baweja @ aol . com ' ; ' gbaxley @ nt . com ' ; ' dabayers @ juno . com ' ;\n",
       "' jbeard @ halff . com ' ; ' bearden . e @ grainger . com ' ; ' tbeaslel @ tuelectric . com ' ;\n",
       "' kayebeatty @ aol . com ' ; ' triciabeaudreau @ hotmail . com ' ;\n",
       "' abeckley @ executrain - dal . com ' ; ' scott . r . bellamy @ marshmc . com ' ;\n",
       "' chiaggie @ aol . com ' ; ' dbenefield @ merit . com ' ; ' bryan @ dalmac . com ' ;\n",
       "' bennie @ flash . net ' ; ' bergerd @ earthlink . net ' ; ' ted . e . bernard @ ac . com ' ;\n",
       "' sberry @ nortel . com ' ; ' jody . bingham @ ps . net ' ; ' bobird @ att . com ' ;\n",
       "' keithbird @ yahoo . com ' ; ' mbish @ nortel . com ' ; ' dawn . bitar @ ps . net ' ;\n",
       "' bittners @ swbell . net ' ; ' akbjerke @ postoffice . swbell . net ' ;\n",
       "' michael . blahitka @ intervoice - brite . com ' ; ' blairsl @ juno . com ' ;\n",
       "' bnlblake @ flash . net ' ; ' gbock 2 @ excite . com ' ; ' bobb 761 @ worldnet . att . net ' ;\n",
       "' jbond @ genuity . com ' ; ' bonsai 2 @ flash . net ' ; ' bonerhk @ earthlink . net ' ;\n",
       "' warrenlb @ aol . com ' ; ' dbb @ sa - inc . com ' ; ' lynnbottlinger @ hotmail . com ' ;\n",
       "' dkboughton @ home . com ' ; ' mbouma @ pgbpike . com ' ; ' bowden \\_ rap @ msn . com ' ;\n",
       "' jfbowen @ swbell . net ' ; ' cbowersl @ airmail . net ' ; ' scott . bowers @ eds . com ' ;\n",
       "' mbag 92 @ aol . com ' ; ' lorna @ . com ' ;\n",
       "' sheryl . bradley @ eds . com ' ; ' andybradshaw @ home . com ' ; ' bramlett @ home . com ' ;\n",
       "' mwbranch @ aol . com ' ; ' tbrandish @ bigfoot . com ' ; ' kbrannon @ flash . net ' ;\n",
       "' bebe - tx @ mindspring . com ' ; ' devere @ flash . net ' ; ' lgbrennan @ earthlink . net ' ;\n",
       "' nicole @ dalmac . com ' ; ' tmbreeze @ gte . net ' ; ' gwb 2 @ flash . net ' ;\n",
       "' john @ smithsummers . com ' ; ' bmbrinkl @ aol . com ' ; ' nateb 7899 @ aol . com ' ;\n",
       "' melissabrooks @ mindspring . com ' ; ' rhbrooks @ vartec . net ' ;\n",
       "' bbrooks @ sbair . com ' ; ' dbrosey @ airmail . net ' ; ' bbrown @ micron . com ' ;\n",
       "' klbo 2 @ cs . com ' ; ' erich . browne @ central . sun . com ' ; ' deniseb @ ticnet . com ' ;\n",
       "' jbrozovi @ usa . alcatel . com ' ; ' bruckm @ airmail . net ' ;\n",
       "' bbruton @ scan - direct . com ' ; ' david . a . bryant @ bigfoot . com ' ;\n",
       "' ccb @ nortelnetworks . com ' ; ' jnkbull @ netzero . com ' ; ' burchta 330 @ aol . com ' ;\n",
       "' drburdenjr @ aol . com ' ; ' jburnett @ foxsports . net ' ; ' haleburr @ aol . com ' ;\n",
       "' burrow @ nortel . ca ' ; ' rbl 419 @ aol . com ' ; ' mikebusch @ mail . com ' ;\n",
       "' cbyrum @ goodmanfamily . com ' ; ' calkfamf @ home . com ' ;\n",
       "' kcameron @ yahoo - inc . com ' ; ' jsmiley @ pisd . edu ' ; ' jajasoup @ aol . com ' ;\n",
       "' laurie . canning @ ericsson . com ' ; ' jjcantwell @ worldnet . att . net ' ;\n",
       "' djcarr @ texas . net ' ; ' richardjcarroll @ yahoo . com ' ; ' drviv @ yahoo . com ' ;\n",
       "' rob @ startech . org ' ; ' tcarson 98 @ yahoo . com ' ; ' brandacarter @ microlabs . com ' ;\n",
       "' lee \\_ carter @ seha . com ' ; ' dcarter 768 @ aol . com ' ; carter , rhonda ;\n",
       "' todd . carter @ fnc . fujitsu . com ' ; ' cwc 68 @ swbell . net ' ; ' jcash @ firstam . com ' ;\n",
       "' tcastellanos @ usa . net ' ; ' wcaudi @ concentric . net ' ;\n",
       "' cavanaug @ gustafson . com ' ; ' cschamberlin @ mindspring . com ' ;\n",
       "' ebeth @ airmail . net ' ; ' cookie \\_ chambers @ pagenet . com ' ;\n",
       "' jchamp 5626 @ aol . com ' ; ' sherriel @ flash . net ' ; ' smchamp @ dhc . net ' ;\n",
       "' svchandl @ garlandisd . net ' ; ' fectac @ aol . com ' ; ' chris . chastain @ ey . com ' ;\n",
       "' ccchatham @ aol . com ' ; ' kevin . chilcoat @ fritolay . com ' ; ' mattc @ dallas . net ' ;\n",
       "' jchoc @ msn . com ' ; ' shannon @ thechristianfamily . com ' ;\n",
       "' christian @ medicine . tamu . edu ' ; ' jcipolla @ hotmail . com ' ; ' dclark @ dhc . net ' ;\n",
       "' sclark @ dhc . net ' ; ' mclary @ elux . com ' ; ' brad @ bigl 2 sports . com ' ;\n",
       "' clemmons @ home . com ' ; ' beth 2047 @ aol . com ' ; ' acoble @ cisco . com ' ;\n",
       "' jjcoburn @ aol . com ' ; ' dbclaw @ hotmail . com ' ; ' matt \\_ cole @ yahoo . com ' ;\n",
       "' cac 75442 @ aol . com ' ; ' jcoll 75442 @ aol . com ' ; ' jorubycol @ aol . com ' ;\n",
       "' collins 587 @ hotmail . com ' ; ' condoaggie @ aol . com ' ; ' swcbox @ aol . com ' ;\n",
       "' crcandmac @ aol . com ' ; ' crcook @ gte . net ' ; ' martha \\_ cook @ publicis - usa . com ' ;\n",
       "' mustrdsd @ flash . net ' ; ' acooper 401 @ aol . com ' ; ' jcooper 95 @ yahoo . com ' ;\n",
       "' karen . m . cope @ dal . frb . org ' ; ' kellyandamy @ sprintmail . com ' ;\n",
       "' vc 4445 @ earthlink . com ' ; ' brendyc @ aol . com ' ; ' kdcornell @ compuserve . com ' ;\n",
       "' sc 93 @ hotmail . com ' ; ' bcorrell @ aol . com ' ; ' mcortino @ swbell . net ' ;\n",
       "' cowan 95 @ aol . com ' ; ' coxl 997 @ yahoo . com ' ; ' monarch @ usa . net ' ;\n",
       "' jason \\_ cox @ hotmail . com ' ; ' lacoyne @ flashcom . net ' ; ' garycl 2345 @ aol . com ' ;\n",
       "' julesag 95 @ flash . net ' ; ' crawfordsl @ cdm . com ' ; ' phantom 495 @ aol . com ' ;\n",
       "' ccriswel @ pisd . edu ' ; ' sec @ inetinc . com ' ; ' holly . a . cromack @ ac . com ' ;\n",
       "' kcudlipp @ arimail . net ' ; ' mrculp @ home . com ' ; ' cathy . cupps @ eds . com ' ;\n",
       "' curranc @ diamtech . com ' ; ' andyc @ gwmail . plano . gov ' ;\n",
       "' lindsay \\_ daigle @ yahoo . com ' ; ' ag 93 whoop @ hotmail . com ' ;\n",
       "' dtddtd 444 @ aol . com ' ; ' edaniel @ flash . net ' ; ' tamidarby @ home . com ' ;\n",
       "' cagladan @ usa . net ' ; ' smitadas @ ix . netcom . com ' ; ' cgwd 94 @ aol . com ' ;\n",
       "' kay . daugherty 3 @ gte . net ' ; ' bob \\_ daughrity @ cabp . com ' ;\n",
       "' aggiel 984 @ juno . com ' ; ' jdd . rad @ gte . net ' ; ' riverl @ flash . net ' ;\n",
       "' rogercdavis @ home . com ' ; ' stefaniedavis @ yahoo . com ' ; ' bamadavis @ aol . com ' ;\n",
       "' dawsonsix @ aol . com ' ; ' heather @ icsi . net ' ; ' stephanie \\_ s \\_ day @ compusa . com ' ;\n",
       "' cdelarios @ home . com ' ; ' cdeangulo @ msn . com ' ; ' ldeardurff @ mckinneyisd . net ' ;\n",
       "' mdeardurff 62 @ msn . com ' ; ' victor . de . hoyos @ fritolay . com ' ;\n",
       "' mrichmnd @ ix . netcom . com ' ; ' kelly 95 ag @ aol . com ' ; ' ivan 53 @ aol . com ' ;\n",
       "' bdempsey @ dnaent . com ' ; ' macdeth @ swbell . net ' ; ' allandl @ airmail . net ' ;\n",
       "' deweesw @ ttc . com ' ; ' tgd @ ffhm . com ' ; ' to \\_ ronda @ airmail . net ' ;\n",
       "' jmditrapani @ nextlink . com ' ; ' ledlugos @ aol . com ' ; ' dlugosch @ home . com ' ;\n",
       "' melvausa @ netscape . net ' ; ' aol 93775 @ dlemail . itg . ti . com ' ;\n",
       "' chas 41 @ airmail . net ' ; ' mdorsett @ uni - bell . org ' ;\n",
       "' m - mdouglas @ worldnet . att . net ' ; ' dovers @ sprintmail . com ' ;\n",
       "' lorip @ rsn . hp . com ' ; ' michelle . drawert @ gte . net ' ; ' tisdalel @ flash . net ' ;\n",
       "' sdrotma @ pisd . edu ' ; ' ndsouza @ unt . edu ' ; ' gary . dubois @ pizzahut . com ' ;\n",
       "' madudko @ aol . com ' ; ' fduewall @ wmcobb . com ' ; ' jduffy @ cisco . com ' ;\n",
       "' dduffy @ mis - world . com ' ; ' blakey @ flash . net ' ; ' greg \\_ dupree @ bigfoot . com ' ;\n",
       "' michael . duran @ ps . net ' ; ' g - durham @ ti . coin ' ; ' crma @ flash . net ' ;\n",
       "' travisdye @ home . com ' ; ' earnshaw @ flash . net ' ; ' mechols @ fastlane . net ' ;\n",
       "' jason . eggl @ indsys . ge . com ' ; ' reicher @ tell . net ' ; ' eiland @ ti . com ' ;\n",
       "' tome @ gwmail . plano . gov ' ; ' s \\_ elliott @ hotmail . com ' ; ' lellis @ ch 2 m . com ' ;\n",
       "' tedcarles @ earthlink . net ' ; ' stephen . elmendorf @ teradyne . com ' ;\n",
       "' bembrey @ ccgmail . com ' ; ' rempey @ waymark . net ' ; ' mengels @ airmail . net ' ;\n",
       "' mentrop @ yahoo . com ' ; ' jepps @ intecom . com ' ; ' donerb @ home . com ' ;\n",
       "' lucy \\_ vsi @ ix . netcom . com ' ; ' mike @ estesfinancial . com ' ; ' setch @ onebox . com ' ;\n",
       "' kathyeudy @ yahoo . com ' ; ' pevers @ home . com ' ; ' kewing @ airmail . net ' ;\n",
       "' deon . b . fair @ ac . com ' ; ' lisalynn 98 @ hotmail . com ' ; ' drjuiceplus @ home . com ' ;\n",
       "' sfaseler @ lg . com ' ; ' rfeldman @ ascend . com ' ; ' j . felkner @ worldnet . att . net ' ;\n",
       "' jferguso @ mony . com ' ; ' roger . ferguson @ fluor . com ' ; ' jtferrarol @ home . com ' ;\n",
       "' tfiedler @ flash . net ' ; ' ififfick @ hharchitects . com ' ;\n",
       "' davidfinley 82 @ yahoo . com ' ; ' duke . fisher @ wcom . com ' ;\n",
       "' j \\_ fishero @ yahoo . com ' ; ' dfitzgerald @ mesquiteisd . org ' ; ' lpfitz @ wt . net ' ;\n",
       "' 102372 . 2423 @ compuserve . com ' ; ' fleck @ concentric . net ' ;\n",
       "' jannet @ dallas . net ' ; ' fleitman @ msn . com ' ; ' samf @ dallas . net ' ;\n",
       "' rjflorio @ worldnet . att . net ' ; ' gulfview @ gateway . net ' ;\n",
       "' d - forbes @ rtis . ray . com ' ; ' bgfort @ earthlink . net ' ; ' clfoster @ airmail . net ' ;\n",
       "' r . foster @ prelude . com ' ; ' gfoyt @ hdrinc . com ' ; ' sfrancis @ everdream . com ' ;\n",
       "' halgodal @ flash . net ' ; ' hedgehogracing @ msn . com ' ;\n",
       "' steve . french @ aggies . org ' ; ' jfreytag @ airmail . net ' ; ' blakef @ msn . com ' ;\n",
       "' michael . froman @ octel . com ' ; ' afruhling @ metasolv . com ' ;\n",
       "' fuentes @ noval . net ' ; ' hiroko @ rsn . hp . com ' ; ' fulkfamily @ home . com ' ;\n",
       "' ron . fuqua @ usa . alcatel . com ' ; ' debra . galarde @ eds . com ' ;\n",
       "' txhoss @ ix . netcom . com ' ; ' jared . galloway @ fnc . fujitsu . com ' ;\n",
       "' aubree . garrett @ fnc . fujitsu . com ' ; ' toniandmikeg @ home . com ' ;\n",
       "' cwgary @ ont . com ' ; ' 2 ags @ flash . net ' ; ' sgaster @ kpmg . com ' ;\n",
       "' zgoner @ airmail . net ' ; ' dgedeon @ vectrix . com ' ;\n",
       "' tara . gedeon @ brannforbes . com ' ; ' jcjones @ rsn . hp . com ' ;\n",
       "' tageo @ mindspring . com ' ; ' tgeorge @ flash . net ' ; ' teresagill @ email . com ' ;\n",
       "' rglover @ halff . com ' ; ' dfglynnl @ msn . com ' ; ' mgolaboff @ eqrworld . com ' ;\n",
       "' judie \\_ good @ yahoo . com ' ; ' gorski @ aggies . com ' ; ' algough @ yahoo . com ' ;\n",
       "' neilgould @ usa . net ' ; ' sallsgraham @ hotmail . com ' ; ' pgranier @ portal . com ' ;\n",
       "' begrant @ flash . net ' ; ' rgrantham @ worldnet . att . net ' ; ' tgravett @ wans . net ' ;\n",
       "' chris \\_ greer @ hp . com ' ; ' hgreer @ alldata . net ' ; ' chrisg @ micrografx . com ' ;\n",
       "' donindfw @ ix . netcom . com ' ; ' dan @ productcentre . com ' ;\n",
       "' jgroce @ lasercomm - inc . com ' ; ' juggernaut @ connect . net ' ;\n",
       "' katie \\_ gruebel @ hotmail . com ' ; ' amynurse @ hotmail . com ' ; ' bag 2 @ airmail . net ' ;\n",
       "' kenneth \\_ guest @ hp . com ' ; ' jgump @ mail . arco . com ' ; ' wylie . gunter @ eds . com ' ;\n",
       "' tim . gutschlag @ fnc . fujitsu . com ' ; ' guzmans @ home . com ' ;\n",
       "' cherihaby @ home . com ' ; ' julie \\_ halloran @ yahoo . com ' ;\n",
       "' chaltom @ conedrive . textron . com ' ; ' hamelb 21 @ ont . com ' ;\n",
       "' talana 99 @ hotmail . com ' ; ' greg . hanks @ hanksbrokerage . com ' ;\n",
       "' rharbin @ aris . com ' ; ' carrie . l . hardy @ fritolay . com ' ;\n",
       "' scott \\_ harkins @ msn . com ' ; ' jharper @ flash . net ' ;\n",
       "' jharrington @ sagetelecom . net ' ; ' roynteri @ mail . com ' ;\n",
       "' steveharrod @ msn . com ' ; ' hartfield @ ti . com ' ;\n",
       "' terry . k . hartzog @ us . arthurandersen . com ' ; ' kharvey @ pcrrent . com ' ;\n",
       "' marji . j . harvey @ mail . sprint . com ' ; ' b - haskettl @ ti . com ' ;\n",
       "' kelly \\_ hayes @ harwoodmarketing . com ' ; ' mhaye @ amkor . com ' ;\n",
       "' alanh @ alliancearch . com ' ; ' mheath @ nextlink . com ' ; ' mheffner @ home . com ' ;\n",
       "' jets @ ti . com ' ; ' glenn @ hc - cpa . com ' ; ' toddmel @ texoma . net ' ;\n",
       "' kimberly . henderson @ ey . com ' ; ' dah 85 @ mindspring . com ' ;\n",
       "' shenley @ flash . net ' ; ' rahennessy @ earthlink . com ' ; ' jherblin @ onramp . net ' ;\n",
       "' carynlynn @ msn . com ' ; ' nascar @ mikeh . net ' ; ' travis @ herringangus . com ' ;\n",
       "' anandted @ msn . com ' ; ' dherron @ pisd . edu ' ; ' mitchherzog @ yahoo . com ' ;\n",
       "' sc \\_ hester @ hotmail . com ' ; ' jheye @ psp . com ' ; ' mhickox @ fiskrob . com ' ;\n",
       "' lori @ efficient . com ' ; ' phinojos @ micro . honeywell . com ' ;\n",
       "' d . hirt @ dialogic . com ' ; ' danetami @ airmail . net ' ; ' randyhobert @ msn . com ' ;\n",
       "' blakekimhodge @ yahoo . com ' ; ' will @ . com ' ;\n",
       "' choldrid @ airmail . net ' ; ' jnh @ ti . com ' ; ' tholman @ gte . net ' ;\n",
       "' sholmeso 0 @ msn . com ' ; ' jholstea @ jpi . com ' ; ' sholton @ ticnet . com ' ;\n",
       "' holyoak @ flash . net ' ; ' phorton @ usa . alcatel . com ' ; ' scott @ horton . net ' ;\n",
       "' thowes @ mail . arco . com ' ; ' chad @ tice . com ' ; ' hugghins @ gte . net ' ;\n",
       "' jhummel @ memc . com ' ; ' markhunt @ bigfoot . com ' ; ' thehurd @ hex . net ' ;\n",
       "' b - hutcheson @ ti . com ' ; ' j . r . iacoponelli @ mciworld . com ' ;\n",
       "' billirish @ hotmail . com ' ; ' czjkjj @ msn . com ' ;\n",
       "' pat . jackson @ fnc . fujitsu . com ' ; ' sjackson @ opsos . net ' ; ' ararat @ flash . net ' ;\n",
       "' stevejames @ home . com ' ; ' chellejanow @ hotmail . com ' ;\n",
       "' asmith \\_ scuba @ yahoo . com ' ; ' bjehu @ yahoo . com ' ;\n",
       "' ashlea \\_ jenkins @ hotmail . com ' ; ' cjenson @ flash . net ' ; ' mljideas @ home . com ' ;\n",
       "' slj @ waymark . net ' ; ' rjolly \\_ 1 @ yahoo . com ' ; ' bsjones 50 @ hotmail . com ' ;\n",
       "' craig - charlottejones @ worldnet . att . net ' ; ' danny @ lanyx . com ' ;\n",
       "' mattjones @ ccgmail . com ' ; ' wjones @ clearsail . net ' ;\n",
       "' sjordan 3 @ compuserve . com ' ; ' ryanjust @ hotmail . com ' ;\n",
       "' chip @ cscfinancial . com ' ; ' lkcbsl @ home . com ' ; ' makall 5 @ flash . net ' ;\n",
       "' twk @ msg . ti . com ' ; ' mkaplan @ augustmail . com ' ; ' ckarlik @ swbell . net ' ;\n",
       "' shafia 30 @ hotmail . com ' ; ' mlkawas @ hotmail . com ' ; ' chipk @ nortel . com ' ;\n",
       "' markkelley \\_ wurzburg @ yahoo . com ' ; ' mkelly 2575 @ juno . com ' ;\n",
       "' danken 8765 @ home . com ' ; ' wolfcamp @ hotmail . com ' ; ' mjkereluk @ msn . com ' ;\n",
       "' ckerley @ apclink . com ' ; ' lkerr @ evl . net ' ; ' r . kessel @ ssss . com ' ;\n",
       "' dkessler @ waymark . net ' ; ' mkessner @ hotmail . com ' ; ' troykey @ peoplepc . com ' ;\n",
       "' akilpatrick @ kurion . com ' ; ' kings 2 @ flash . net ' ; ' kingsr @ home . com ' ;\n",
       "' jkingston @ ti . com ' ; ' skirchner @ worldnet . att . net ' ;\n",
       "' chuck @ digitalpilot . com ' ; ' michael . kleppe @ ericsson . com ' ;\n",
       "' jklouda @ flash . net ' ; ' dennis \\_ kniery @ hp . com ' ; ' sschulz @ mail . smu . edu ' ;\n",
       "' lakohler @ raytheon . com ' ; ' james . kornegay @ eds . com ' ; ' knrkrause @ aol . com ' ;\n",
       "' kckuddes @ altavista . com ' ; ' sakula @ flash . net ' ; ' skutchin @ leaelliott . com ' ;\n",
       "' bladdusaw @ ti . **com ' ; ' 103745 .** 342 @ compuserve . com ' ; ' mellake @ yahoo . com ' ;\n",
       "' paul . lake @ ps . net ' ; ' slakie @ texas . net ' ; ' jplane @ gte . net ' ;\n",
       "' mlangloys @ aol . com ' ; ' rlanicek @ home . com ' ;\n",
       "' barrett . lankford @ painewebber . com ' ; ' mlara @ pisd . edu ' ; ' gsl @ msn . com ' ;\n",
       "' mikepl @ bnr . ca ' ; ' klavergne @ earthling . net ' ;\n",
       "' winner @ sportsstandings . com ' ; ' mlecrone @ aol . com ' ; ' banglee @ ti . com ' ;\n",
       "' coyote 97 @ swbell . net ' ; ' robertlee @ poboxes . com ' ;\n",
       "' j . lemmons @ worldnet . att . net ' ; ' lesliel @ airmail . net ' ; ' lerich @ flash . net ' ;\n",
       "' rlessmann @ home . com ' ; ' elethe @ gte . net ' ; ' mikelew @ nortelnetworks . com ' ;\n",
       "' lewisr 691 @ home . com ' ; ' laliefer @ aol . com ' ; ' hkl 5320 @ dcccd . edu ' ;\n",
       "' gmlz @ msg . ti . com ' ; ' blightsey @ systemdesk . com ' ;\n",
       "' 74464 . 2612 @ compuserve . com ' ; ' jlind 2402 @ aol . com ' ;\n",
       "' dlindstrom @ icidallas . com ' ; ' glinebaugh @ prodigy . net ' ;\n",
       "' eflinhoff @ aol . com ' ; ' the . lisewskys @ prodigy . net ' ;\n",
       "' mlish @ kennedywilson . com ' ; ' heidident @ aol . com ' ; ' katie 96 ag @ yahoo . com ' ;\n",
       "' john \\_ london @ acs - inc . com ' ; ' ro 219 @ aol . com ' ; ' balott @ aol . com ' ;\n",
       "' wadel @ swbell . net ' ; ' tglovell @ onramp . net ' ; ' tlovell @ ticnet . com ' ;\n",
       "' rmlowry 4 @ yahoo . com ' ; ' mploya @ ti . com ' ; ' aggie 97 @ hotmail . com ' ;\n",
       "' jlugo @ rhaaia . com ' ; ' klukshin @ kpmg . com ' ; ' dluna @ raltron . com ' ;\n",
       "' ped @ nortel . ca ' ; ' clyons @ metasolv . com ' ; ' paulandkarin @ msn . com ' ;\n",
       "' rlyttons @ aol . com ' ; ' emaas 94 @ yahoo . com ' ; ' spam . bait @ worldnet . att . net ' ;\n",
       "' neardal @ airmail . net ' ; ' mmachesney @ aol . com ' ; ' netaces @ airmail . net ' ;\n",
       "' richard . maddox @ mci . com ' ; ' betty . magee @ homesbybetty . com ' ;\n",
       "' jmagrude @ jpi . com ' ; ' pxm @ msg . ti . com ' ; ' tracemajor @ hotmail . com ' ;\n",
       "' mmalakoff @ aol . com ' ; ' judy . j . manning @ fritolay . com ' ;\n",
       "' norris @ mantooth . com ' ; ' marchand \\_ darryl @ msn . com ' ; ' nrm 2000 @ hotmail . com ' ;\n",
       "' mike . marino @ usoncology . com ' ; ' branonmarsh @ hotmail . com ' ;\n",
       "' rmartin @ coserv . net ' ; ' dmason @ highpointtravel . com ' ; ' seanab @ gte . net ' ;\n",
       "' debm 394 @ aol . com ' ; ' mathews - amy @ yahoo . com ' ; ' ags 84 @ aol . com ' ;\n",
       "' equestlnm @ excite . com ' ; ' mjmattson @ home . com ' ; ' bmatulal @ airmail . net ' ;\n",
       "' ilvjesus @ flash . net ' ; ' kmay 4001 @ aol . com ' ; ' cmayber @ pisd . edu ' ;\n",
       "' jasonmayes @ earthlink . net ' ; ' jimbobq 88 @ aol . com ' ; ' mccaff @ anet - dfw . com ' ;\n",
       "' bmccainl 62 @ aol . com ' ; ' almac @ wans . net ' ; ' mike . mcdonald @ ey . com ' ;\n",
       "' dhm @ mcdowelllabel . com ' ; ' tmcevoy @ wordware . com ' ;\n",
       "' trisheeey @ hotmail . com ' ; ' bmcgrego @ metrogroup . com ' ; ' rmckee @ ti . com ' ;\n",
       "' jim \\_ mcmahan @ ctxmort . com ' ; ' bmcmillan @ motion - dynamics . com ' ;\n",
       "' pmeggs @ aol . com ' ; ' mendezn @ nortelnetworks . com ' ; ' jenabug @ flash . net ' ;\n",
       "' m \\_ mentzer @ hotmail . com ' ; ' sandymergen @ hotmail . com ' ;\n",
       "' smerrill @ flash . net ' ; ' tiffany \\_ merrill @ yahoo . com ' ;\n",
       "' jmersiovsky @ metasolv . com ' ; ' emetting @ hntb . com ' ;\n",
       "' sue \\_ middleton @ juno . com ' ; ' jbm 326 @ aol . com ' ; ' barbmiller @ qualtx . com ' ;\n",
       "' michael \\_ c . \\_ miller @ ac . com ' ; ' miller @ dallas . net ' ;\n",
       "' hdjemills @ earthlink . net ' ; ' jmills @ dallas . net ' ;\n",
       "' dminaldi @ contactdallas . com ' ; ' rminney @ entercon . com ' ; ' mlm @ ti . com ' ;\n",
       "' jenmizar @ yahoo . com ' ; ' moonaggie @ cs . com ' ; ' jason @ aggies . org ' ;\n",
       "' pipkins @ gateway . net ' ; ' danny . morris @ mciworld . com ' ;\n",
       "' jcipolla @ hotmail . com ' ; ' cmorse @ waymark . net ' ; ' aggietx @ swbell . net ' ;\n",
       "' m \\_ muecke @ hotmail . com ' ; ' jeff . mundt @ wcom . com ' ; ' amurphy 96 @ hotmail . com ' ;\n",
       "' jmurphy 4 @ hotmail . com ' ; ' dannym @ churchrealty . com ' ; ' cmyers @ mycon . com ' ;\n",
       "' greg @ lsil . com ' ; ' jnlzaza @ earthlink . net ' ; ' erinsneedham @ hotmail . com ' ;\n",
       "' tpneeley @ worldnet . att . net ' ; ' jnelson @ source . com ' ;\n",
       "' jnerwich @ mindspring . com ' ; ' goonet @ hotmail . com ' ; ' rpnew @ aol . com ' ;\n",
       "' jeff . newton @ fritolay . com ' ; ' chrisgnichols @ yahoo . com ' ;\n",
       "' r . niedenfuehr @ worldnet . att . net ' ; ' nielsonc @ sprynet . com ' ;\n",
       "' rniesen @ ti . com ' ; ' jnobll @ jcpenney . com ' ; ' timcathy @ flash . net ' ;\n",
       "' merkicpa @ gte . net ' ; ' rnorris @ joefunkconstr . com ' ; ' aggiel @ airmail . net ' ;\n",
       "' cnorton @ brierley . com ' ; ' janicen @ architeriors . com ' ;\n",
       "' nnowik @ mhagroup . com ' ; ' toconnor @ varo . com ' ; ' melody . oliver @ eds . com ' ;\n",
       "' s - oliverl @ ti . com ' ; ' adrienneolsen @ hotmail . com ' ; ' roneal @ ins - inc . com ' ;\n",
       "' tfonofrio @ aol . com ' ; ' b - orem @ rtis . ray . com ' ; ' jetpilot @ sprintmail . com ' ;\n",
       "' orr @ caprock . net ' ; ' kwunsch @ ci . garland . tx . us ' ; ' jott @ rsn . hp . com ' ;\n",
       "' tamc 66 @ aol . com ' ; ' atm 97 @ aol . com ' ; ' powen 94 @ yahoo . com ' ;\n",
       "' yohanp @ netscape . net ' ; ' palitza @ att . net ' ; ' dpalmer @ pisd . edu ' ;\n",
       "' cparker @ garlandpower - light . org ' ; ' wanda . parker @ wjpenterprises . com ' ;\n",
       "' tamu 97 @ airmail . net ' ; ' jpatoskie @ home . com ' ;\n",
       "' judy . peacock @ worldnet . att . net ' ; ' david . a . pearl @ travelers . com ' ;\n",
       "' katie @ lifelinehomehealth . com ' ; ' ppedison @ aol . com ' ;\n",
       "' lpeichel @ nortelnetworks . com ' ; ' mpell @ uswebcks . com ' ; ' dannyp 83 @ gte . net ' ;\n",
       "' david \\_ perry @ 3 com . com ' ; ' picardl 999 @ hotmail . com ' ;\n",
       "' friscoattorney @ aol . com ' ; ' dphillips @ pfsoutsourcing . com ' ;\n",
       "' cpierce @ lee - eng . com ' ; ' kurtpifer @ hotmail . com ' ;\n",
       "' stephen \\_ pilcher @ yahoo . com ' ; ' wpindar 3 @ email . msn . com ' ;\n",
       "' pingenot @ gte . net ' ; ' pinzon @ nortel . com ' ; ' mwpiper @ onramp . net ' ;\n",
       "' dpitts @ acm . org ' ; ' ppjp @ airmail . net ' ; ' mplumer @ synhrgy . com ' ;\n",
       "' randy @ pogueinc . com ' ; ' jerrypoin @ home . com ' ;\n",
       "' tony . pollacia @ fritolay . com ' ; ' tammypon @ hmhs . com ' ;\n",
       "' kent @ webdelight . net ' ; ' cporter @ nortelnetworks . com ' ;\n",
       "' sporter @ texas . net ' ; ' porterfields @ prodigy . net ' ; ' texas \\_ anm @ yahoo . com ' ;\n",
       "' poteet @ dmans . com ' ; ' billpowello 4 @ home . com ' ; ' ammy 5 @ aol . com ' ;\n",
       "' joshp @ thisco . com ' ; ' marykpowl @ syscodallas . com ' ;\n",
       "' prater 2 @ earthlink . net ' ; ' dprattl @ home . com ' ; ' d - presley @ tamu . edu ' ;\n",
       "' musicgrl 68 @ aol . com ' ; ' pauld @ homemail . com ' ; ' kpruitt @ gasequipment . com ' ;\n",
       "' pprzada @ aol . com ' ; ' beckyp @ bmisystems . com ' ; ' impurdy @ 5 pillars . com ' ;\n",
       "' mrpyatt @ airmail . net ' ; ' jlqjr @ gte . net ' ; ' scradford @ aol . com ' ;\n",
       "' melissa \\_ ragan @ richards . com ' ; ' eric . ragle @ cisco - eagle . com ' ;\n",
       "' maheswaran \\_ rajasekharan @ i 2 . com ' ; ' kikiaggie @ webcombo . net ' ;\n",
       "' mramsey @ unitedad . com ' ; ' michael . rasmussen @ ps . net ' ; ' j - read @ tamu . edu ' ;\n",
       "' jlreadpa @ aol . com ' ; ' reasor @ rsn . hp . com ' ; ' reck @ gateway . net ' ;\n",
       "' cindy . redman @ eds . com ' ; ' dreed @ is . arco . com ' ; ' reedl 00 @ msn . com ' ;\n",
       "' tdreed @ airmail . net ' ; ' solutionhr @ aol . com ' ; ' jreeves @ agave . com ' ;\n",
       "' cremmele @ aol . com ' ; ' rrestivo @ eversoft . com ' ; ' erice 8 @ aol . com ' ;\n",
       "' sanrice @ aol . com ' ; ' ct \\_ richard @ hotmail . com ' ; ' mrichard @ arcmail . com ' ;\n",
       "' krichards @ acsdallas . com ' ; ' paula . g . richmond @ fritolay . com ' ;\n",
       "' jrickman @ hppclaw . com ' ; ' tlrigby @ home . com ' ; ' kcriggs @ yahoo . com ' ;\n",
       "' mrightm @ mail . arco . com ' ; ' jriha @ businessobjects . com ' ; ' rrinker @ wtd . net ' ;\n",
       "' rippees @ swbell . net ' ; ' rippel @ utdallas . edu ' ; ' writchie @ ci . irving . tx . us ' ;\n",
       "' bradyroberts @ hotmail . com ' ; ' laserbaker @ worldnet . att . net ' ;\n",
       "' frobert @ aol . com ' ; ' krisaggi @ aol . com ' ; ' ker @ ti . com ' ; ' roco @ nortel . com ' ;\n",
       "' kjroeker @ airmail . net ' ; ' jimroseo 3 @ home . com ' ;\n",
       "' suzanne \\_ ross @ campbellsoup . com ' ; ' jim \\_ rountree @ logiclsales . com ' ;\n",
       "' eddie . rueffer @ mci . com ' ; ' srupprecht @ chubb . com ' ; ' jennyr @ wtd . net ' ;\n",
       "' kimed @ hotmail . com ' ; ' jryan @ uswebcks . com ' ; ' emsalazar 25 @ hotmail . com ' ;\n",
       "' k - salazarl @ ti . com ' ; ' jlsales @ waymark . net ' ;\n",
       "' msanchez @ mckinneytexas . org ' ; ' steven . sarkissian @ painwebber . com ' ;\n",
       "' danna @ nortelnetworks . com ' ; ' tsawyers @ aol . com ' ; ' scheumack @ juno . com ' ;\n",
       "' dschmidt @ camozzi - usa . com ' ; ' pschmidt @ connect . net ' ; ' tammyms @ yahoo . com ' ;\n",
       "' nathan . schockmel @ usa . alcatel . com ' ; ' kschoenhals @ metasolv . com ' ;\n",
       "' schuelerjs @ aol . com ' ; ' diana \\_ p \\_ seal @ email . mobil . com ' ;\n",
       "' pkemper @ 3 dfx . com ' ; ' sherri . seeger @ wylieisd . net ' ; ' maseeley @ avaya . com ' ;\n",
       "' tseely @ attglobal . net ' ; ' rshackelford @ home . com ' ;\n",
       "' shannons @ websurfer . net ' ; ' jtshannon @ ticnet . com ' ;\n",
       "' loren . sharkey @ brinker . com ' ; ' rehan @ computer . org ' ; ' gryffynn @ aol . com ' ;\n",
       "' xosloren @ ti . com ' ; ' roger . shellenberger @ exscol . exch . eds . com ' ;\n",
       "' kshelton @ amfm . com ' ; ' samleannshields @ aol . com ' ; ' sbshin @ evl . net ' ;\n",
       "' alsikes @ pbsj . com ' ; ' glenn \\_ silva @ gmaccm . com ' ;\n",
       "' frank . silva @ industrialrisk . com ' ; ' simmonds @ marykay . com ' ;\n",
       "' atmrick @ aol . com ' ; ' isivin @ aol . com ' ; ' rskaggs @ hksinc . com ' ;\n",
       "' bskalberg @ aol . com ' ; ' todd @ nkn . net ' ; ' dsmart @ dttus . com ' ;\n",
       "' amy . l . smith @ eds . com ' ; ' egsmith @ home . com ' ;\n",
       "' john \\_ charles \\_ smith @ compuserve . com ' ; ' john - h - smith @ raytheon . com ' ;\n",
       "' ksmith @ kma - rjfs . com ' ; ' shanda @ wans . net ' ;\n",
       "' michael . smith @ usa . alcatel . com ' ; ' rjsmith @ minutemaid . com ' ;\n",
       "' rsmith @ metasolv . com ' ; ' dick \\_ smith @ pagenet . com ' ;\n",
       "' agent \\_ maroon @ hotmail . com ' ; ' enviropure @ home . com ' ; ' unclewil @ home . com ' ;\n",
       "' jsmitherman @ cinemark . com ' ; ' jim \\_ snow @ millipore . com ' ;\n",
       "' gpsparks @ hotmail . com ' ; ' tspo 92891 @ aol . com ' ; ' dspencer @ dbssystems . com ' ;\n",
       "' lspielel @ txu . com ' ; ' txagl 987 @ aol . com ' ; ' g - stanford @ raytheon . com ' ;\n",
       "' petgeoguru @ hotmail . com ' ; ' jstara @ arcmail . com ' ;\n",
       "' kgstavin @ garlandisd . net ' ; ' tbstebbins @ aol . com ' ; ' jsteck @ ti . com ' ;\n",
       "' steffler @ mindspring . com ' ; ' gsteglich @ home . com ' ; ' shane @ computer . org ' ;\n",
       "' sastephen @ home . com ' ; ' dereks @ us . ibm . com ' ; ' tsteudtner @ aol . com ' ;\n",
       "' jill . stevens @ risd . org ' ; ' jnelwyn @ aol . com ' ; ' dons @ gwmail . plano . gov ' ;\n",
       "' mstewart 70 @ aol . com ' ; ' pstewart 86 @ hotmail . com ' ;\n",
       "' rastewartl 2 @ hotmail . com ' ; ' msticken @ airmail . net ' ;\n",
       "' cstockmoe @ yahoo . com ' ; ' k - stokes @ tamu . edu ' ; ' michael \\_ stone @ nt . com ' ;\n",
       "' mcstrietzel @ home . com ' ; ' staceys @ omassociates . com ' ;\n",
       "' sstroth @ glitsch . com ' ; ' h - r . strozewski @ worldnet . att . net ' ;\n",
       "' astryker @ swbell . net ' ; ' macecs @ hotmail . com ' ; ' smsturgeon @ kpmg . com ' ;\n",
       "' sullivan 22 @ home . com ' ; ' normas @ airmail . net ' ; ' wswanson @ cyberramp . net ' ;\n",
       "' rtank 20 @ aol . com ' ; ' matt \\_ tanner @ txu . com ' ; ' ftargac @ hotmail . com ' ;\n",
       "' taylorgr @ nortel . com ' ; ' taylorl @ airmail . net ' ; ' aggierob @ hotmail . com ' ;\n",
       "' teresa . taylor @ st . com ' ; ' ticaw @ hotmail . com ' ; ' wst @ flash . net ' ;\n",
       "' caceett @ hotmail . com ' ; ' punt 3442 @ aol . com ' ; ' chris . t @ prodigy . net ' ;\n",
       "' denise . thatcher @ eds . com ' ; ' mjthed @ earthlink . net ' ;\n",
       "' brandon . theis @ eds . com ' ; ' steve \\_ thelen @ cushwake . com ' ;\n",
       "' arthur . thomas @ ace - ina . com ' ; ' tthomas @ cooperinst . org ' ;\n",
       "' dthomps 2 @ pisd . edu ' ; ' nthompson @ swst . com ' ; ' psthompson @ mindspring . com ' ;\n",
       "' rthompso @ kofax . com ' ; ' tierney \\_ thompson @ winston - school . org ' ;\n",
       "' b - tinker @ ti . com ' ; ' tipp @ airmail . net ' ; ' atokarz @ usa . alcatel . com ' ;\n",
       "' bevtoney @ aol . com ' ; ' kstowery @ mindspring . com ' ;\n",
       "' patrick . traubert @ tripointglobal . com ' ; ' heidigigem 96 @ yahoo . com ' ;\n",
       "' tiffanytrox @ yahoo . com ' ; ' dddtruitt @ juno . com ' ;\n",
       "' tschetter @ worldnet . att . net ' ; ' 9 mtucker @ home . com ' ;\n",
       "' oxymomloree @ aol . com ' ; ' cturner @ entest . net ' ; ' aisdal @ aol . com ' ;\n",
       "' rachturney @ yahoo . com ' ; ' gulteig @ starrunner . net ' ; ' gutay @ airmail . net ' ;\n",
       "' hutay @ yahoo . com ' ; ' valls @ earthlink . net ' ; ' hvanpelt @ msn . net ' ;\n",
       "' dwv @ vanderburg . org ' ; ' timv @ cheerful . com ' ; ' r - cvaughn @ juno . com ' ;\n",
       "' annette . vela @ homecomings . com ' ; ' jvetkoetter @ pipeline . com ' ;\n",
       "' diane \\_ vetter @ hotmail . com ' ; ' vicem @ hdvest . com ' ; ' jennifer @ vilches . org ' ;\n",
       "' pvilches @ home . com ' ; ' spvill @ flash . net ' ; ' ssv @ attglobal . net ' ;\n",
       "' kristi . l . vitek @ fritolay . com ' ; ' voltin @ airmail . net ' ; ' wagso 0 @ yahoo . com ' ;\n",
       "' walessc @ nortelnetworks . com ' ; ' bwalker @ fmtinv . com ' ;\n",
       "' brian . walker @ exchange - point . com ' ; ' deanwalker @ computer . org ' ;\n",
       "' ken . walker @ mscsoftware . com ' ; ' shannon . wallace @ usa . net ' ;\n",
       "' b \\_ wallace @ prodigy . net ' ; ' wwallenl @ airmail . net ' ; ' mkwalle @ yahoo . com ' ;\n",
       "' twaller @ excite . com ' ; ' drwaller @ hotmail . com ' ;\n",
       "' maxwalters @ worldnet . att . net ' ; ' kwalzel @ cisco . com ' ;\n",
       "' julie . warden @ mhmr . state . tx . us ' ; ' warner @ ont . com ' ; ' watersco @ flash . net ' ;\n",
       "' julie . watkins @ eds . com ' ; ' apwo 397 @ juno . com ' ; ' patricia . watsono 2 @ ey . com ' ;\n",
       "' jwebb @ dalsemi . com ' ; ' jkwebb 41 @ gateway . net ' ; ' debbiew @ mciworld . com ' ;\n",
       "' robin - w @ juno . com ' ; ' sally \\_ welch @ excite . com ' ;\n",
       "' twelch @ nortelnetworks . com ' ; ' awaller @ csc . com ' ;\n",
       "' gregwemhoener @ home . com ' ; ' susanwempe @ hotmail . com ' ; ' jwest 78 @ aol . com ' ;\n",
       "' mike . west @ usa . alcatel . com ' ; ' joannwest @ earthlink . net ' ;\n",
       "' mweynand @ flash . net ' ; ' weynandken @ johndeere . com ' ; ' mweynand @ flash . net ' ;\n",
       "' mmw @ airmail . net ' ; ' ewheal @ jcpenney . com ' ; ' swhite @ tx . pathnet . net ' ;\n",
       "' txag 93 sw @ flash . net ' ; ' chris @ iex . com ' ; ' wiegard @ nortel . com ' ;\n",
       "' cindyw @ pobox . com ' ; ' jbouldin @ teleteam . com ' ; ' mike @ paragon - tx . com ' ;\n",
       "' rtwilkinsn @ aol . com ' ; ' jerriw @ arn . net ' ; ' joeaggie 93 @ msn . com ' ;\n",
       "' kristilw @ swbell . net ' ; ' a 50 @ flash . net ' ; ' rkwbdw 580 @ cs . com ' ;\n",
       "' kswccw @ swbell . net ' ; ' designpath @ sprynet . com ' ; ' drw 58 ag @ aol . com ' ;\n",
       "' wilsonaggies @ home . com ' ; ' normabautista @ worldnet . att . net ' ;\n",
       "' skisheri @ aol . com ' ; ' emajo . wilson @ gte . net ' ; ' jenkonquin @ aol . com ' ;\n",
       "' wvicw @ aol . com ' ; ' teresa . wood @ st . com ' ; ' lwood 963 @ flash . net ' ;\n",
       "' tcwoolley @ writeme . com ' ; ' sworsham @ supermovers . com ' ;\n",
       "' brad \\_ worth @ csicontrols . com ' ; ' wrightson . family @ gte . net ' ;\n",
       "' dwu @ tqtx . com ' ; ' dlwylie @ swbell . net ' ; ' christa . yakel @ sap - ag . de ' ;\n",
       "' yarbrough \\_ james @ hotmail . com ' ; ' suzan @ guyyork . com ' ;\n",
       "' karen \\_ znoj @ merck . com '\n",
       "subject : \" red , white and blue out \"\n",
       "subject : the osu game and aggie spirit\n",
       "this just in ags ! if you are going to the osu game on sept 22 , a red , white\n",
       "& blue out is being planned , just like the maroon out games for the osu\n",
       "game . ags , what better statement can we aggies make , than to celebrate the love\n",
       "and support for our country ' s freedom , and our patriotic nature , than this\n",
       "way :\n",
       "imagine . . . the fightin ' texas aggie band playing \" the star spangled banner , \"\n",
       "playing military drills as they walk around the stadium , and we celebrate\n",
       "our love for our country , and our support for all the heroes , alive and\n",
       "deceased . color assignments are as follows :\n",
       "3 rd deck : red\n",
       "2 nd deck : white lst deck : blue\n",
       "pass the word on . . . . we have 1 1 / 2 weeks ! ! the spirit of america , and the aggie spirit is still alive . god bless\n",
       "america , the free nation . pass it on .\n",
       "\n",
       "### Document 2579\n",
       "\n",
       "Subject: transport on koch , beginning wednesday\n",
       "christina ,\n",
       "the indian springs plant delivery into hpl will be taken out of service for\n",
       "repiping work effective wednesday morning the 15 th . ena will be moving\n",
       "texas desk purchases via koch gateway from teco polk co . plant ( 12780 ) to\n",
       "midcon needville ( 6350 ) . i have confirmed available capacity with koch and\n",
       "made sure that the it agreement is evergreen and will work for these\n",
       "purposes . please let me know what i can do to ensure that the nomination process works\n",
       "as smoothly as possible . i will work with daren and liz to make sure that\n",
       "the purchase and sales tickets are moved to koch . please let me know who is\n",
       "responsible for the transport usage ticket on koch , if necessary , i will put\n",
       "the ticket in this afternoon . we will be responsible for a \\$ . 02 commodity\n",
       "fee , aca of \\$ . **0022 and 1 .** 6 % fuel ( monetizes to \\$ . 09 approximately ) to move\n",
       "these volumes . ena will be receiving 84 , 987 dth at the plant and delivering 83 , 627 into\n",
       "midcon texas . we also have the option to deliver by displacement into midcon\n",
       "at goodrich or edna , or displace deliveries to koch bayside . please let me know if you have any questions or need anything further from\n",
       "me . mary\n",
       "ext . 35251\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_search(\"elephant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398686a6",
   "metadata": {},
   "source": [
    "# Optional 1\n",
    "\n",
    "If you have access to OpenAI's API, and you can afford the charges (it will be about 15 US-cents, or about $0.25),\n",
    "try using it for creating the embedding for the query and the emails.\n",
    "\n",
    "Otherwise, try using another embedding from the MTEB leaderboard.\n",
    "\n",
    "Your code will be almost the same as for the bag-of-words model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2eb3cd",
   "metadata": {},
   "source": [
    "# Optional 2\n",
    "\n",
    "Python's standard library includes `difflib` which has a function `difflib.get_close_matches()` \n",
    "\n",
    "This lets you do approximate matching of words instead of exact matching. It's an alternative to \n",
    "doing lemmatization. Try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c15bf54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
